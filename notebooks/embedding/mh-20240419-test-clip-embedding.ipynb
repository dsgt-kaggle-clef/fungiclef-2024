{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maximilianheil/fungiclef-2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /home/maximilianheil/fungiclef-2024/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import pyspark.ml as ml\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "import fungiclef.embedding.transforms as trans\n",
    "from fungiclef.utils import get_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/19 22:21:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/04/19 22:21:47 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fungiclef-dev.us-central1-a.c.dsgt-clef-2024.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>fungi_clef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x798fb0283df0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = get_spark(**{\n",
    "    \"spark.sql.parquet.enableVectorizedReader\": False, \n",
    "})\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- data: binary (nullable = true)\n",
      " |-- observationID: long (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- day: double (nullable = true)\n",
      " |-- countryCode: string (nullable = true)\n",
      " |-- locality: string (nullable = true)\n",
      " |-- taxonID: double (nullable = true)\n",
      " |-- scientificName: string (nullable = true)\n",
      " |-- kingdom: string (nullable = true)\n",
      " |-- phylum: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- order: string (nullable = true)\n",
      " |-- family: string (nullable = true)\n",
      " |-- genus: string (nullable = true)\n",
      " |-- specificEpithet: string (nullable = true)\n",
      " |-- taxonRank: string (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      " |-- level0Gid: string (nullable = true)\n",
      " |-- level0Name: string (nullable = true)\n",
      " |-- level1Gid: string (nullable = true)\n",
      " |-- level1Name: string (nullable = true)\n",
      " |-- level2Gid: string (nullable = true)\n",
      " |-- level2Name: string (nullable = true)\n",
      " |-- ImageUniqueID: string (nullable = true)\n",
      " |-- Substrate: string (nullable = true)\n",
      " |-- rightsHolder: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- CoorUncert: double (nullable = true)\n",
      " |-- Habitat: string (nullable = true)\n",
      " |-- class_id: integer (nullable = true)\n",
      " |-- MetaSubstrate: string (nullable = true)\n",
      " |-- poisonous: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3723"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gcs_parquet_path = \"gs://dsgt-clef-fungiclef-2024/dev/dev_train/\"\n",
    "\n",
    "#gcs_parquet_path = \"gs://dsgt-clef-fungiclef-2024/data/parquet/\"\n",
    "#input_folder = f\"DF20\"\n",
    "\n",
    "\n",
    "\n",
    "dev_df = spark.read.parquet(gcs_parquet_path)\n",
    "dev_df.printSchema()\n",
    "dev_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['month',\n",
       " 'locality',\n",
       " 'scientificName',\n",
       " 'order',\n",
       " 'family',\n",
       " 'genus',\n",
       " 'specificEpithet',\n",
       " 'species',\n",
       " 'level1Name',\n",
       " 'level2Name',\n",
       " 'Substrate',\n",
       " 'Habitat',\n",
       " 'MetaSubstrate',\n",
       " 'poisonous']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_cols = ['class', 'phylum', 'Latitude', 'Longitude', 'CoorUncert', 'txt_data', 'taxonRank', 'class_id', 'kingdom', 'image_path', 'data', 'observationID', 'year', 'ImageUniqueID', 'rightsHolder', 'day', 'countryCode', 'taxonID', 'level0Gid', 'level0Name', 'level1Gid', 'level2Gid']\n",
    "relevant_columns = [col for col in  dev_df.columns if col not in ignore_cols]\n",
    "relevant_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_floats_to_int(df, list_of_cols):\n",
    "    for col in list_of_cols:\n",
    "        df = df.withColumn(col, dev_df[col].cast('int'))\n",
    "    return df\n",
    "\n",
    "dev_df = cast_floats_to_int(dev_df, ['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " txt_data | month 9, locality SmÃ¸rmosen, scientificName Russula nigricans (Bull.) Fr., order Russulales, family Russulaceae, genus Russula, specificEpithet nigricans, species Russula adusta, level1Name Hovedstaden, level2Name Herlev, Substrate soil, Habitat bog, MetaSubstrate jord, poisonous 0 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# concat all txt data in one col txt_data\n",
    "\n",
    "transformed_cols = [f.concat(f.lit(f\"{col_name} \"), f.col(col_name).cast(\"string\")).alias(col_name) for col_name in relevant_columns]\n",
    "\n",
    "dev_df = dev_df.withColumn(\"txt_data\", f.concat_ws(\", \", *transformed_cols))\n",
    "\n",
    "dev_df.select(\"txt_data\").show(1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:================================================>        (16 + 3) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+-------+\n",
      "|           txt_token|            txt_data|len_txt_token|raw_len|\n",
      "+--------------------+--------------------+-------------+-------+\n",
      "|[month, 9,, local...|month 9, locality...|           47|    408|\n",
      "|[month, 9,, local...|month 9, locality...|           47|    381|\n",
      "|[month, 9,, local...|month 9, locality...|           47|    408|\n",
      "|[month, 9,, local...|month 9, locality...|           47|    381|\n",
      "|[month, 7,, local...|month 7, locality...|           47|    408|\n",
      "|[month, 9,, local...|month 9, locality...|           47|    412|\n",
      "|[month, 7,, local...|month 7, locality...|           47|    408|\n",
      "|[month, 11,, loca...|month 11, localit...|           46|    379|\n",
      "|[month, 11,, loca...|month 11, localit...|           46|    379|\n",
      "|[month, 9,, local...|month 9, locality...|           46|    380|\n",
      "+--------------------+--------------------+-------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(outputCol=\"txt_token\")\n",
    "tokenizer.setInputCol(\"txt_data\")\n",
    "\n",
    "tokenized_df = tokenizer.transform(dev_df).select('txt_token', 'txt_data')\n",
    "\n",
    "tokenized_df = tokenized_df.withColumn(\"len_txt_token\", f.size(f.col(\"txt_token\")))\n",
    "tokenized_df = tokenized_df.withColumn(\"raw_len\", f.length(f.col(\"txt_data\")))\n",
    "\n",
    "tokenized_df.sort('len_txt_token', ascending=False).show(10, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all txt data in one col txt_data\n",
    "\n",
    "transformed_cols = [f.concat(f.lit(f\"{col_name} \"), f.col(col_name).cast(\"string\")).alias(col_name) for col_name in relevant_columns]\n",
    "\n",
    "dev_df = dev_df.withColumn(\"txt_data\", f.concat_ws(\", \", *relevant_columns))\n",
    "\n",
    "dev_df.select(\"txt_data\").show(1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/19 22:22:28 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# Init DINOv2 wrapper\n",
    "clip = trans.WrappedCLIP(input_cols=[\"data\",\"txt_data\"], output_col=\"transformed_data\")\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = ml.Pipeline(stages=[clip]) #, dctn\n",
    "\n",
    "# Fit pipeline to DF\n",
    "model = pipeline.fit(dev_df)\n",
    "\n",
    "# Apply the model to transform the DF\n",
    "transformed_df = model.transform(dev_df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (85 > 77). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (125 > 77). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (92 > 77). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (93 > 77). Running this sequence through the model will result in indexing errors\n",
      "{'input_ids': tensor([[49406,  1924,   279,   267,  9202,   696, 39144,    82, 18449,   909,\n",
      "          8479,   267,  4338,  1788,  1981, 14562,   647,  1898,   718,   664,\n",
      "           553,   570,  1157,   533,   263,   532,   522,   330,  5376,  7343,\n",
      "         19837,   267,  8796,  3581,   261,   603,  1544,  3581,   267,   273,\n",
      "           271,   272,   275,   267,  2191,   647,   534,  9469,   267,  1315,\n",
      "           647,  1898, 38153,   267, 38161, 14562,   647,  1898,   718,   267,\n",
      "          1969,  1788,  7219,   599,   339,   664,   553,   570,  1157,   533,\n",
      "           267,  6330, 14562,   647,  1898,   718,   664,   553,   570,  1157,\n",
      "           533,   267,  2931,   272,  1981,  2192, 18890,  2656,   973,   267,\n",
      "          2931,   273,  1981,   651,  1177,  1356,   267,  1783, 28758,  9216,\n",
      "           267, 11747,  6780, 17447,   263,   593, 39031,  1765,   879,   537,\n",
      "          2262,   691,   879,  4682,  2361,  5249, 13157,    65, 28758,  2482,\n",
      "           323,   267, 37131,   271, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]]), 'pixel_values': tensor([[[[ 0.7625,  1.0982,  1.2296,  ...,  0.0325, -0.0405,  0.9668],\n",
      "          [ 0.9084,  1.4486,  1.2880,  ...,  0.0763,  0.9084,  1.9011],\n",
      "          [ 0.5289,  1.5070,  1.0836,  ...,  0.6311,  0.8938,  1.6968],\n",
      "          ...,\n",
      "          [-0.4784, -0.1134, -0.2156,  ...,  0.9814,  0.9960,  0.6895],\n",
      "          [-0.5952, -0.7704, -0.1134,  ...,  0.7625,  0.6019,  0.7187],\n",
      "          [-0.3762, -0.7266, -0.7120,  ...,  0.7771,  0.6895,  0.9522]],\n",
      "\n",
      "         [[ 0.2890,  0.5591,  0.3640,  ..., -0.6265, -0.6415,  0.4240],\n",
      "          [ 0.3490,  0.9493,  0.6341,  ..., -0.7316,  0.6341,  1.8948],\n",
      "          [-0.0412,  0.9793,  0.2439,  ..., -0.1763,  0.6792,  1.6997],\n",
      "          ...,\n",
      "          [-0.8516, -0.3714, -0.4914,  ...,  0.9043,  0.9193,  0.5891],\n",
      "          [-0.9717, -1.0317, -0.4314,  ...,  0.6491,  0.4991,  0.5891],\n",
      "          [-0.7466, -1.0017, -1.0617,  ...,  0.6041,  0.5291,  0.7992]],\n",
      "\n",
      "         [[ 0.1693,  0.4395,  0.1693,  ..., -0.6412, -0.8403,  0.0982],\n",
      "          [ 0.2262,  0.6244,  0.1409,  ..., -0.7408,  0.1693,  1.3211],\n",
      "          [-0.0582,  0.4821, -0.4564,  ..., -0.5275, -0.0298,  0.8945],\n",
      "          ...,\n",
      "          [-0.7692, -0.3711, -0.5133,  ...,  0.6244,  0.7097,  0.4395],\n",
      "          [-0.8830, -1.0394, -0.4990,  ...,  0.5106,  0.3684,  0.4537],\n",
      "          [-0.6555, -0.9825, -1.0394,  ...,  0.4821,  0.3968,  0.6386]]]])}\n",
      "{'input_ids': tensor([[49406,  1924,   280,   267,  9202,   696,   978, 17483,    81,   617,\n",
      "          2249,   267,  4338,  1788,  1981, 35811, 26143,   697,   709, 20511,\n",
      "           263,  6029,  5376,  5512,  5276,  2191, 35811,  1767,  5962,   267,\n",
      "          1315, 35811,  1767, 38153,   267, 38161, 35811, 26143,   267,  1969,\n",
      "          1788,  7219,   599,   339,   697,   709, 20511,   267,  6330, 35811,\n",
      "         26143, 16882,  2745,   267,  2931,   272,  1981,   606, 19515,  1363,\n",
      "          1181,   267,  2931,   273,  1981,  1223, 29950,   267,  1783, 28758,\n",
      "          9216,   267, 11747, 28318,   267,  5249, 13157,    65, 28758,  2482,\n",
      "           323,   267, 37131,   271, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'pixel_values': tensor([[[[-0.5076, -0.2886,  0.0325,  ..., -0.4200, -1.2375, -1.1061],\n",
      "          [-0.7850, -0.5514, -0.0259,  ..., -0.9164, -1.0915, -1.0039],\n",
      "          [-0.4200, -0.2740, -0.1572,  ..., -1.3543, -1.0185, -0.9164],\n",
      "          ...,\n",
      "          [-0.2448, -0.2448, -0.1134,  ..., -0.5806, -0.6244, -0.6390],\n",
      "          [-0.0550, -0.1864, -0.4054,  ..., -0.6974, -0.6682, -0.5952],\n",
      "          [-0.1134, -0.3616, -0.5222,  ..., -0.6682, -0.6390, -0.6244]],\n",
      "\n",
      "         [[-0.8666, -0.5215, -0.2663,  ...,  0.3640, -0.1313, -0.1313],\n",
      "          [-0.8816, -0.6115, -0.2363,  ..., -0.1463, -0.1313, -0.0262],\n",
      "          [ 0.0038,  0.0939, -0.0262,  ..., -0.7166, -0.4764, -0.0562],\n",
      "          ...,\n",
      "          [ 0.6642,  0.3340,  0.3190,  ..., -0.6415, -0.7316, -0.7316],\n",
      "          [ 0.9793,  0.6041,  0.3190,  ..., -0.7616, -0.7916, -0.7016],\n",
      "          [ 0.9343,  0.5441,  0.3790,  ..., -0.7466, -0.7616, -0.7466]],\n",
      "\n",
      "         [[-1.0110, -0.8972, -0.7123,  ..., -0.4137, -0.9967, -0.8830],\n",
      "          [-1.1105, -1.0252, -0.7266,  ..., -0.8830, -0.9256, -0.8261],\n",
      "          [-0.4706, -0.4848, -0.6697,  ..., -1.3807, -1.1247, -0.8545],\n",
      "          ...,\n",
      "          [-0.7266, -0.6981, -0.5275,  ..., -0.9256, -0.9683, -0.9399],\n",
      "          [-0.4848, -0.5701, -0.6412,  ..., -0.9825, -0.9825, -0.9541],\n",
      "          [-0.4564, -0.6412, -0.5986,  ..., -0.9256, -0.9256, -0.9541]]]])}\n",
      "{'input_ids': tensor([[49406,  1924,   279,   267,  9202,   696,  1556,   643, 14770, 19261,\n",
      "           909,  8479,   267,  4338,  1788,  1981, 35811, 26143,   697,   709,\n",
      "         20511,   263,  6029,  5376,  5512,  5276,  2191, 35811,  1767,  5962,\n",
      "           267,  1315, 35811,  1767, 38153,   267, 38161, 35811, 26143,   267,\n",
      "          1969,  1788,  7219,   599,   339,   697,   709, 20511,   267,  6330,\n",
      "         35811, 26143, 16882,  2745,   267,  2931,   272,  1981,  2192, 18890,\n",
      "          2656,   973,   267,  2931,   273,  1981,  1556,   643, 14770,   267,\n",
      "          1783, 28758,  9216,   267, 11747, 39031,  1765,   879, 17447,   270,\n",
      "         20014,   267,  5249, 13157,    65, 28758,  2482,   323,   267, 37131,\n",
      "           271, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'pixel_values': tensor([[[[-0.0405, -0.6244,  0.3537,  ...,  1.0398,  0.9668,  0.4267],\n",
      "          [ 0.1347, -0.4492,  0.1639,  ...,  0.8355,  0.5727,  0.1931],\n",
      "          [ 0.1347, -0.2740,  0.0179,  ...,  0.1639, -0.1864, -0.1718],\n",
      "          ...,\n",
      "          [ 0.2223,  0.1347,  0.0471,  ...,  0.8792,  0.1055,  0.4121],\n",
      "          [ 0.4121,  0.3099,  0.3099,  ...,  0.9084,  0.4705,  0.4413],\n",
      "          [ 0.3537,  0.2077,  0.0033,  ...,  0.8938,  0.3391,  0.4997]],\n",
      "\n",
      "         [[ 0.1539, -0.3864,  0.6041,  ...,  1.3545,  1.2344,  0.5441],\n",
      "          [ 0.2740, -0.2363,  0.3790,  ...,  1.0694,  0.7542,  0.2289],\n",
      "          [ 0.2890, -0.0862,  0.1839,  ...,  0.1689, -0.2213, -0.2813],\n",
      "          ...,\n",
      "          [ 0.6191,  0.4841,  0.3790,  ...,  0.8593, -0.0412,  0.1689],\n",
      "          [ 0.5441,  0.3940,  0.3940,  ...,  0.9193,  0.3790,  0.2589],\n",
      "          [ 0.3790,  0.1689, -0.0262,  ...,  0.9043,  0.2890,  0.3490]],\n",
      "\n",
      "         [[-0.0867, -0.3711,  0.9514,  ...,  1.4207,  1.3922,  0.7808],\n",
      "          [-0.1293, -0.3853,  0.6386,  ...,  1.1221,  0.8945,  0.3968],\n",
      "          [-0.2146, -0.3711,  0.3542,  ...,  0.3542,  0.0271, -0.1009],\n",
      "          ...,\n",
      "          [ 0.3968,  0.2688,  0.1266,  ...,  1.1363,  0.1835,  0.2831],\n",
      "          [ 0.4821,  0.3684,  0.3542,  ...,  1.1932,  0.6386,  0.4110],\n",
      "          [ 0.3399,  0.2262,  0.0840,  ...,  1.2074,  0.5817,  0.5390]]]])}\n",
      "{'input_ids': tensor([[49406,  1924,   279,   267,  9202,   696,    65, 42495,  1356,   909,\n",
      "          8479,   267,  4338,  1788,  1981, 35811, 26143,    78, 13207, 15691,\n",
      "          1684,   263, 10710,  5376,  5512,  5276,  2191, 35811,  1767,  5962,\n",
      "           267,  1315, 35811,  1767, 38153,   267, 38161, 35811, 26143,   267,\n",
      "          1969,  1788,  7219,   599,   339,    78, 13207, 15691,  1684,   267,\n",
      "          6330, 35811, 26143,    78, 13207, 15691,  1684,   267,  2931,   272,\n",
      "          1981,  7536, 42495,    75,   973,   267,  2931,   273,  1981,   534,\n",
      "            73,   810,   267,  1783, 28758,  9216,   267, 11747,  2262,   691,\n",
      "           879, 17447,   267,  5249, 13157,    65, 28758,  2482,   323,   267,\n",
      "         37131,   271, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'pixel_values': tensor([[[[ 0.1493,  0.0325,  0.0179,  ..., -0.2448, -0.5222, -0.6828],\n",
      "          [ 0.0763,  0.0471,  0.0179,  ..., -0.5222, -0.5368, -0.5076],\n",
      "          [-0.1426, -0.0696, -0.0696,  ..., -1.1499, -0.8580, -0.6828],\n",
      "          ...,\n",
      "          [-0.0405, -0.2156, -0.3470,  ..., -1.3251, -1.3689, -1.3543],\n",
      "          [ 0.0325, -0.2010, -0.3762,  ..., -1.3835, -1.2959, -1.3105],\n",
      "          [ 0.0325, -0.2156, -0.3908,  ..., -1.4711, -1.2959, -1.0769]],\n",
      "\n",
      "         [[ 0.6041,  0.5591,  0.5291,  ..., -0.3714, -0.7616, -0.8816],\n",
      "          [ 0.5141,  0.5141,  0.4390,  ..., -0.7166, -0.8216, -0.7616],\n",
      "          [ 0.2439,  0.3040,  0.2589,  ..., -1.3019, -1.0767, -0.8516],\n",
      "          ...,\n",
      "          [-0.1163, -0.1913, -0.1763,  ..., -1.3319, -1.3769, -1.3319],\n",
      "          [-0.0712, -0.1913, -0.2213,  ..., -1.3919, -1.3019, -1.3169],\n",
      "          [-0.0562, -0.1913, -0.2663,  ..., -1.4369, -1.2718, -1.0467]],\n",
      "\n",
      "         [[ 0.6528,  0.5675,  0.5248,  ...,  0.0271, -0.3426, -0.5559],\n",
      "          [ 0.5532,  0.5106,  0.4395,  ..., -0.4422, -0.5417, -0.5701],\n",
      "          [ 0.2688,  0.3115,  0.2688,  ..., -1.1105, -0.9114, -0.7408],\n",
      "          ...,\n",
      "          [-0.4137, -0.4848, -0.4990,  ..., -1.1105, -1.1674, -1.1389],\n",
      "          [-0.3853, -0.4848, -0.5417,  ..., -1.1674, -1.0963, -1.1247],\n",
      "          [-0.3711, -0.4848, -0.5844,  ..., -1.2243, -1.0821, -0.8688]]]])}\n",
      "24/04/19 21:37:33 WARN BlockManager: Putting block rdd_25_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (93) must match the size of tensor b (77) at non-singleton dimension 1\n",
      ".\n",
      "24/04/19 21:37:33 WARN BlockManager: Putting block rdd_25_2 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (92) must match the size of tensor b (77) at non-singleton dimension 1\n",
      ".\n",
      "24/04/19 21:37:33 WARN BlockManager: Putting block rdd_25_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (125) must match the size of tensor b (77) at non-singleton dimension 1\n",
      ".\n",
      "24/04/19 21:37:33 WARN BlockManager: Putting block rdd_25_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (85) must match the size of tensor b (77) at non-singleton dimension 1\n",
      ".\n",
      "24/04/19 21:37:33 WARN BlockManager: Block rdd_25_0 could not be removed as it was not found on disk or in memory\n",
      "24/04/19 21:37:33 WARN BlockManager: Block rdd_25_3 could not be removed as it was not found on disk or in memory\n",
      "24/04/19 21:37:33 WARN BlockManager: Block rdd_25_2 could not be removed as it was not found on disk or in memory\n",
      "24/04/19 21:37:33 WARN BlockManager: Block rdd_25_1 could not be removed as it was not found on disk or in memory\n",
      "24/04/19 21:37:33 ERROR Executor: Exception in task 1.0 in stage 6.0 (TID 42)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (93) must match the size of tensor b (77) at non-singleton dimension 1\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/04/19 21:37:33 ERROR Executor: Exception in task 3.0 in stage 6.0 (TID 44)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (125) must match the size of tensor b (77) at non-singleton dimension 1\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/04/19 21:37:33 ERROR Executor: Exception in task 2.0 in stage 6.0 (TID 43)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (92) must match the size of tensor b (77) at non-singleton dimension 1\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/04/19 21:37:33 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 41)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (85) must match the size of tensor b (77) at non-singleton dimension 1\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/04/19 21:37:33 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 41) (fungiclef-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (85) must match the size of tensor b (77) at non-singleton dimension 1\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/04/19 21:37:33 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job\n",
      "24/04/19 21:37:33 WARN TaskSetManager: Lost task 2.0 in stage 6.0 (TID 43) (fungiclef-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (92) must match the size of tensor b (77) at non-singleton dimension 1\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/04/19 21:37:33 WARN TaskSetManager: Lost task 1.0 in stage 6.0 (TID 42) (fungiclef-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (93) must match the size of tensor b (77) at non-singleton dimension 1\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/04/19 21:37:33 WARN TaskSetManager: Lost task 3.0 in stage 6.0 (TID 44) (fungiclef-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (125) must match the size of tensor b (77) at non-singleton dimension 1\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/04/19 21:37:33 WARN BlockManager: Putting block rdd_25_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/04/19 21:37:33 WARN BlockManager: Block rdd_25_7 could not be removed as it was not found on disk or in memory\n",
      "24/04/19 21:37:33 WARN BlockManager: Putting block rdd_25_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/04/19 21:37:33 WARN BlockManager: Block rdd_25_5 could not be removed as it was not found on disk or in memory\n",
      "24/04/19 21:37:33 WARN BlockManager: Putting block rdd_25_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/04/19 21:37:33 WARN BlockManager: Block rdd_25_4 could not be removed as it was not found on disk or in memory\n",
      "24/04/19 21:37:33 WARN BlockManager: Putting block rdd_25_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/04/19 21:37:33 WARN TaskSetManager: Lost task 7.0 in stage 6.0 (TID 48) (fungiclef-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 41) (fungiclef-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (85) must match the size of tensor b (77) at non-singleton dimension 1\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/04/19 21:37:33 WARN BlockManager: Block rdd_25_6 could not be removed as it was not found on disk or in memory\n",
      "24/04/19 21:37:33 WARN TaskSetManager: Lost task 4.0 in stage 6.0 (TID 45) (fungiclef-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 41) (fungiclef-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (85) must match the size of tensor b (77) at non-singleton dimension 1\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/04/19 21:37:33 WARN TaskSetManager: Lost task 5.0 in stage 6.0 (TID 46) (fungiclef-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 41) (fungiclef-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (85) must match the size of tensor b (77) at non-singleton dimension 1\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/04/19 21:37:33 WARN TaskSetManager: Lost task 6.0 in stage 6.0 (TID 47) (fungiclef-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 41) (fungiclef-dev.us-central1-a.c.dsgt-clef-2024.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n",
      "    preds = predict_fn(*multi_inputs)\n",
      "  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n",
      "    text_features = outputs.text_embeds\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n",
      "    text_outputs = self.text_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n",
      "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n",
      "    embeddings = inputs_embeds + position_embeddings\n",
      "RuntimeError: The size of tensor a (85) must match the size of tensor b (77) at non-singleton dimension 1\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:288)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n    preds = predict_fn(*multi_inputs)\n  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n    text_features = outputs.text_embeds\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n    text_outputs = self.text_model(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n    embeddings = inputs_embeds + position_embeddings\nRuntimeError: The size of tensor a (85) must match the size of tensor b (77) at non-singleton dimension 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransformed_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mImageUniqueID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspecies\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformed_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/functions.py\", line 799, in predict\n    preds = predict_fn(*multi_inputs)\n  File \"/home/maximilianheil/fungiclef-2024/fungiclef/embedding/transforms.py\", line 192, in predict\n    text_features = outputs.text_embeds\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 1117, in forward\n    text_outputs = self.text_model(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 699, in forward\n    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 227, in forward\n    embeddings = inputs_embeds + position_embeddings\nRuntimeError: The size of tensor a (85) must match the size of tensor b (77) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "transformed_df.select([\"ImageUniqueID\", \"species\", \"transformed_data\"]).show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+\n",
      "|    ImageUniqueID|             species|    transformed_data|\n",
      "+-----------------+--------------------+--------------------+\n",
      "| 2237920504-77555|      Russula adusta|[0.009258113, 0.0...|\n",
      "|  2237966683-5021|      Russula adusta|[0.019642368, 0.0...|\n",
      "| 2237968009-79449|      Russula adusta|[0.0032701602, 0....|\n",
      "| 2238149799-82085|Neoboletus luridi...|[-0.0048931465, 0...|\n",
      "|  2238150033-7827|       Imleria badia|[-0.0015484869, 0...|\n",
      "|2238171177-306291|  Lactarius blennius|[0.003171264, 0.0...|\n",
      "| 2238209494-84510|Neoboletus luridi...|[0.022397494, 0.0...|\n",
      "| 2238331048-11804|  Russula ochroleuca|[-0.014820853, 0....|\n",
      "| 2238428423-15997|  Russula ochroleuca|[0.024092644, 0.0...|\n",
      "| 2238455499-91533|  Lactarius blennius|[0.0208195, 0.047...|\n",
      "+-----------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.select([\"ImageUniqueID\", \"species\", \"transformed_data\"]).show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Init Descrite Cosine Transform wrapper\n",
    "dctn = trans.DCTN(input_col=\"transformed_data\", output_col=\"dctn_data\")\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = ml.Pipeline(stages=[dino, dctn]) #, dctn\n",
    "\n",
    "# Fit pipeline to DF\n",
    "model = pipeline.fit(dev_subset_df)\n",
    "\n",
    "# Apply the model to transform the DF\n",
    "transformed_df = model.transform(dev_subset_df).cache()\n",
    "\n",
    "# Show results\n",
    "transformed_df.select([\"ImageUniqueID\", \"species\", \"transformed_data\"]).show(n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
