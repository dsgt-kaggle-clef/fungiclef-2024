{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "The objective of this notebook is to\n",
    "1. Combine both train / val dataset on the dataset page to make one very big dataset\n",
    "2. For classes that are NOT in the train dataset, label them as unknown \n",
    "3. Convert the images to binary file and add it to the dataframe\n",
    "4. Filter for variables in test set only\n",
    "5. Convert all categorical variables (for both input and output) into numerical variables\n",
    "6. Save everything into a parquet\n",
    "\n",
    "This is built on / should replace the work done on images-to-parquet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF20_train_path = \"../data/FungiCLEF2023_train_metadata_PRODUCTION.csv\" # This is the dev training set page\n",
    "DF21_val_path = \"../data/FungiCLEF2023_val_metadata_PRODUCTION.csv\" # This is the \"validation\" set on the page. We want to use this with additional data for unknown classes\n",
    "public_test_path = \"../data/FungiCLEF2023_public_test_metadata_PRODUCTION.csv\" # Public test set\n",
    "IMG_PATH = \"../data/DF\"\n",
    "\n",
    "DF20_df = pd.read_csv(DF20_train_path)\n",
    "DF21_df = pd.read_csv(DF21_val_path)\n",
    "test_df = pd.read_csv(public_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = pd.concat((DF20_df, DF21_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['observationID', 'month', 'day', 'countryCode', 'locality', 'level0Gid',\n",
       "       'level0Name', 'level1Gid', 'level1Name', 'level2Gid', 'level2Name',\n",
       "       'Substrate', 'Latitude', 'Longitude', 'CoorUncert', 'Habitat',\n",
       "       'image_path', 'filename', 'MetaSubstrate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the metadat we want to keep to train on and potentially for prediction\n",
    "test_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['observationID', 'year', 'month', 'day', 'countryCode', 'locality',\n",
       "       'taxonID', 'scientificName', 'kingdom', 'phylum', 'class', 'order',\n",
       "       'family', 'genus', 'specificEpithet', 'taxonRank', 'species',\n",
       "       'level0Gid', 'level0Name', 'level1Gid', 'level1Name', 'level2Gid',\n",
       "       'level2Name', 'ImageUniqueID', 'Substrate', 'rightsHolder', 'Latitude',\n",
       "       'Longitude', 'CoorUncert', 'Habitat', 'image_path', 'class_id',\n",
       "       'MetaSubstrate', 'poisonous', 'filename'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Additionally, we probably want to include all the phylum, genus, etc. It might be useful for additional training data.\n",
    "train_val_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = list(test_df.keys()) + ['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 'poisonous', 'class_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = train_val_df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "DUMMY_DATE = 361\n",
    "train_val_df.loc[:, 'normalized_day'] = ((train_val_df['month'] - 1) * 30 + train_val_df['day']).fillna(DUMMY_DATE).astype(np.int16, copy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = ['locality', 'level0Gid', 'level1Gid', 'level2Gid', 'Substrate', 'Habitat', 'MetaSubstrate', 'kingdom', 'phylum', 'class',\n",
    "       'order', 'family', 'genus', 'species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is important to save \n",
    "mapping = {}\n",
    "\n",
    "for col in categoricals:\n",
    "    train_val_df.sort_values(by=col, ascending=True, inplace=True)\n",
    "    col_numerical, col_mapping = pd.factorize(train_val_df[col], use_na_sentinel=True)\n",
    "    train_val_df.loc[:, f\"{col}_numerical\"] = col_numerical\n",
    "    mapping[col] = {v: k for k, v in enumerate(col_mapping)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "CATEGORICAL_MAPPING_LOCATION = \"../data/categorical_mapping.pkl\"\n",
    "\n",
    "pickle.dump(mapping, open(CATEGORICAL_MAPPING_LOCATION, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be converted to a script for submission - categorical mapping for test_df\n",
    "\n",
    "test_categoricals = ['locality', 'level0Gid', 'level1Gid', 'level2Gid', 'Substrate', 'Habitat', 'MetaSubstrate']\n",
    "mapping = pickle.load(open(CATEGORICAL_MAPPING_LOCATION, 'rb'))\n",
    "\n",
    "for col in test_categoricals:\n",
    "    test_df.loc[:, col+\"_numerical\"] = test_df[col].apply(lambda x: mapping[col].get(x, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df.to_csv(\"train_val_df.csv\", index=False)\n",
    "train_val_df = pd.read_csv(\"train_val_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7135/7135 [00:07<00:00, 924.61it/s] \n",
      "100%|██████████| 7135/7135 [00:11<00:00, 599.90it/s]\n",
      "100%|██████████| 7135/7135 [00:11<00:00, 646.25it/s]\n",
      "100%|██████████| 7135/7135 [00:12<00:00, 594.26it/s]\n",
      "100%|██████████| 7135/7135 [00:10<00:00, 664.30it/s]\n",
      "100%|██████████| 7135/7135 [00:11<00:00, 625.52it/s]\n",
      "100%|██████████| 7135/7135 [00:11<00:00, 625.90it/s]\n",
      "100%|██████████| 7135/7135 [00:11<00:00, 616.47it/s]\n",
      "100%|██████████| 7135/7135 [00:11<00:00, 636.19it/s]\n",
      "100%|██████████| 7135/7135 [00:12<00:00, 563.38it/s]\n",
      "100%|██████████| 7135/7135 [00:10<00:00, 655.24it/s]\n",
      "100%|██████████| 7135/7135 [00:11<00:00, 617.87it/s]\n",
      "100%|██████████| 7135/7135 [00:11<00:00, 609.77it/s]\n",
      "100%|██████████| 7135/7135 [00:11<00:00, 613.79it/s]\n",
      "100%|██████████| 7135/7135 [00:11<00:00, 603.92it/s]\n",
      "100%|██████████| 7135/7135 [00:11<00:00, 605.97it/s]\n",
      "100%|██████████| 7135/7135 [00:11<00:00, 622.08it/s]\n",
      "100%|██████████| 7135/7135 [00:11<00:00, 606.60it/s]\n",
      "100%|██████████| 7135/7135 [00:14<00:00, 479.07it/s]\n",
      "100%|██████████| 7135/7135 [00:13<00:00, 524.59it/s]\n",
      "100%|██████████| 7135/7135 [00:11<00:00, 604.21it/s]\n",
      "100%|██████████| 7135/7135 [00:12<00:00, 561.18it/s]\n",
      "100%|██████████| 7135/7135 [00:12<00:00, 582.57it/s]\n",
      "100%|██████████| 7135/7135 [00:13<00:00, 530.48it/s]\n",
      "100%|██████████| 7135/7135 [00:12<00:00, 562.86it/s]\n",
      "100%|██████████| 7135/7135 [00:12<00:00, 587.31it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# This keeps on crashing + is super inefficient :( \n",
    "# Spark doesn't work on my local machine either\n",
    "# Need to adapt it with images_to_parquet.py stuff\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "IMG_DIR = \"../data/DF/\"\n",
    "\n",
    "df_records = train_val_df.to_dict(\"records\")\n",
    "CHUNKS = 50\n",
    "CHUNK_SIZE = len(df_records) // CHUNKS\n",
    "\n",
    "for i in range(CHUNKS):\n",
    "    records = []\n",
    "    if i == CHUNKS - 1: \n",
    "        chunk = df_records[i * CHUNK_SIZE:]\n",
    "    else: \n",
    "        chunk = df_records[i * CHUNK_SIZE:(i+1) *CHUNK_SIZE]\n",
    "    for r in tqdm(chunk):\n",
    "        img_name = r['image_path']\n",
    "        if len(img_name.split(\"-\")[0]) == 10:\n",
    "\n",
    "            image_path = IMG_DIR + img_name.replace(\"JPG\", \"jpg\")\n",
    "        else: \n",
    "            image_path = IMG_DIR + img_name\n",
    "        with Image.open(image_path) as im:\n",
    "            r.update({\n",
    "                \"img_height\": im.height,\n",
    "                \"img_widgth\": im.width,\n",
    "                \"data\": im.tobytes()\n",
    "            })\n",
    "        records.append(r)\n",
    "\n",
    "    full_df = pd.DataFrame(records)\n",
    "\n",
    "    _dataset_chunk = pa.Table.from_pandas(full_df, preserve_index=False)\n",
    "    pq.write_table(_dataset_chunk, f\"../data/DF_300_{i}.parquet\") # TODO: To change endpoints where these parquets are stored. But we're using spark anywayz lmao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fungiclef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
