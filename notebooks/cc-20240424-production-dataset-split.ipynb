{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/fungiclef-2024/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fungiclef.utils import get_spark, spark_resource, read_config\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config(path='../fungiclef/config.json')\n",
    "\n",
    "# First, we read the metadata for the dataset and make a proper new one. This will be the single source of truth we use to build the rest of our stuff on\n",
    "# This corresponds to the DF20 dataset\n",
    "TRAIN_METADATA = config[\"gs_paths\"][\"train\"][\"metadata\"]\n",
    "\n",
    "# These two correspond to the DF21 dataset\n",
    "VALID_METADATA = config[\"gs_paths\"][\"val\"][\"metadata\"]\n",
    "TEST_METADATA = config[\"gs_paths\"][\"test\"][\"metadata\"]\n",
    "\n",
    "PRODUCTION_BUCKET = 'gs://dsgt-clef-fungiclef-2024/production/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Making a new train / valid / test set split with metadata file\n",
    "\n",
    "The motivation to this is to make a proper, bigger training set, where unknown classes are also included in the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df = pd.read_csv(TRAIN_METADATA)\n",
    "valid_metadata_df = pd.read_csv(VALID_METADATA)\n",
    "test_metadata_df = pd.read_csv(TEST_METADATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/24 12:06:00 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "full_dataset_pq = config[\"gs_paths\"][\"train_and_test_300px_corrected\"][\"raw_parquet\"]\n",
    "spark = get_spark(**{\n",
    "    \"spark.sql.parquet.enableVectorizedReader\": False, \n",
    "})\n",
    "\n",
    "full_dataset_df = spark.read.parquet(full_dataset_pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "observation_ids = full_dataset_df.select(\"observationID\").toPandas()['observationID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available images:  356770\n"
     ]
    }
   ],
   "source": [
    "print(\"available images: \", len(observation_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  295938\n",
      "valid:  60832\n",
      "test:  60225\n",
      "train + valid:  356770\n"
     ]
    }
   ],
   "source": [
    "print(\"train: \", len(train_metadata_df.observationID))\n",
    "print(\"valid: \", len(valid_metadata_df.observationID))\n",
    "print(\"test: \", len(test_metadata_df.observationID))\n",
    "\n",
    "# It appears that the only images we have are the train and validation datasets. Only metadata is available for the public test set. As such, we need to be a bit wiser in how we split up the data in that case. \n",
    "print(\"train + valid: \", len(train_metadata_df.observationID) + len(valid_metadata_df.observationID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we don't have as much \"new\" data, but we still want to keep the data and stratified, the most straightforward way is to split the valid set (60,832 cases) three-fold.\n",
    "\n",
    "1/3rd will go to the training set (such that it can learn concepts such as unknown), \n",
    "\n",
    "1/3rd will become the validation set (for model tuning)\n",
    "\n",
    "1/3rd will become the sacred held out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Here, we stratify the validation dataset by class_id\n",
    "class_df = valid_metadata_df[['observationID', 'class_id']]\n",
    "\n",
    "# I note that the validation set has numerous repeated observation for some reason. \n",
    "class_df = class_df.drop_duplicates()\n",
    "\n",
    "# For rare classes, we want to keep them within the test set only. \n",
    "test_only_class_ids = class_df.class_id.value_counts()[class_df.class_id.value_counts() < 5].index.tolist()\n",
    "\n",
    "class_df = class_df[~class_df.class_id.isin(test_only_class_ids)]\n",
    "additional_train, new_valid_test = train_test_split(class_df, test_size=0.67, stratify=class_df['class_id'])\n",
    "new_valid, new_test = train_test_split(new_valid_test, test_size=0.5, stratify=new_valid_test['class_id'])\n",
    "new_test = pd.concat((new_test, class_df[class_df.class_id.isin(test_only_class_ids)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional train:  19394\n",
      "new valid:  19788\n",
      "new test:  19746\n"
     ]
    }
   ],
   "source": [
    "# These are the new / additional sets\n",
    "print(\"additional train: \", valid_metadata_df.observationID.isin(additional_train.observationID).sum())\n",
    "print(\"new valid: \", valid_metadata_df.observationID.isin(new_valid.observationID).sum())\n",
    "print(\"new test: \", valid_metadata_df.observationID.isin(new_test.observationID).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df['dataset'] = \"train\"\n",
    "# Label the old validation metadata df with new tag\n",
    "valid_metadata_df.loc[valid_metadata_df.observationID.isin(additional_train.observationID), 'dataset'] = \"train\"\n",
    "valid_metadata_df.loc[valid_metadata_df.observationID.isin(new_valid.observationID), 'dataset'] = \"valid\"\n",
    "valid_metadata_df.loc[valid_metadata_df.observationID.isin(new_test.observationID), 'dataset'] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the two and save to bucket\n",
    "PRODUCTION_BUCKET = 'gs://dsgt-clef-fungiclef-2024/production/'\n",
    "\n",
    "full_metadata_df = pd.concat((train_metadata_df, valid_metadata_df))\n",
    "full_metadata_df.to_csv(PRODUCTION_BUCKET + \"metadata/DF_combined_metadata_full_raw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing categorical variables\n",
    "The following section we will be preprocessing the dataset with categorical columns\n",
    "We will also save this mapping to use for the public dataset / for any subsequent inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are only keeping columns that are relevant either for training or inference. \n",
    "# This includes all the columns that were present in the public test metadata dataset\n",
    "TEST_DF_COLUMNS = ['observationID', 'month', 'day', 'countryCode', 'locality', 'level0Gid',\n",
    "       'level0Name', 'level1Gid', 'level1Name', 'level2Gid', 'level2Name',\n",
    "       'Substrate', 'Latitude', 'Longitude', 'CoorUncert', 'Habitat',\n",
    "       'image_path', 'filename', 'MetaSubstrate']\n",
    "\n",
    "# As well as the overall classification of the fungi (this could potentially be useful as additional training targets)\n",
    "COLUMNS_TO_KEEP = TEST_DF_COLUMNS + ['scientificName', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 'poisonous', 'class_id', 'dataset']\n",
    "\n",
    "# These are the categorical columns we will need to factorize and generate labels for\n",
    "CATEGORICAL_COLUMNS = ['locality', 'level0Gid', 'level1Gid', 'level2Gid', 'Substrate', 'Habitat', 'MetaSubstrate', 'kingdom', 'phylum', 'class',\n",
    "       'order', 'family', 'genus', 'species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_metadata_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m selected_metadata_df \u001b[38;5;241m=\u001b[39m \u001b[43mfull_metadata_df\u001b[49m[COLUMNS_TO_KEEP]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This is important to save \u001b[39;00m\n\u001b[1;32m      4\u001b[0m mapping \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_metadata_df' is not defined"
     ]
    }
   ],
   "source": [
    "selected_metadata_df = full_metadata_df[COLUMNS_TO_KEEP]\n",
    "\n",
    "# This is important to save \n",
    "mapping = {}\n",
    "\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    selected_metadata_df.sort_values(by=col, ascending=True, inplace=True)\n",
    "    col_numerical, col_mapping = pd.factorize(selected_metadata_df[col], use_na_sentinel=False)\n",
    "    selected_metadata_df.loc[:, f\"{col}_text\"] = selected_metadata_df.loc[:, col]\n",
    "    selected_metadata_df.loc[:, col] = col_numerical\n",
    "    mapping[col] = {v: k for k, v in enumerate(col_mapping)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mapping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      5\u001b[0m CATEGORICAL_MAPPING_LOCATION \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./categorical_columns_mapping.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mmapping\u001b[49m, \u001b[38;5;28mopen\u001b[39m(CATEGORICAL_MAPPING_LOCATION, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mapping' is not defined"
     ]
    }
   ],
   "source": [
    "# It is vital to save all the mapping for the categorical columns for inference later.\n",
    "# P.S. This is saved in /production/metadata. \n",
    "import pickle\n",
    "\n",
    "CATEGORICAL_MAPPING_LOCATION = \"./categorical_columns_mapping.pkl\"\n",
    "\n",
    "pickle.dump(mapping, open(CATEGORICAL_MAPPING_LOCATION, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, it is vital to remap the unknown class from -1 to another positive integer. Otherwise it will be hard to train and harder to debug.\n",
    "import numpy as np\n",
    "\n",
    "UNKNOWN_CLASS = 1604\n",
    "selected_metadata_df['class_id'] = np.where(selected_metadata_df.class_id==-1, UNKNOWN_CLASS, selected_metadata_df.class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metadata_df.to_csv(PRODUCTION_BUCKET + \"metadata/DF_combined_metadata_mapped_columns.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pair up with embeddings\n",
    "Now that we have a single metadata dataframe as our single source of truth, we will match the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metadata_df = pd.read_csv(PRODUCTION_BUCKET + \"metadata/DF_combined_metadata_mapped_columns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observationID</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>countryCode</th>\n",
       "      <th>locality</th>\n",
       "      <th>level0Gid</th>\n",
       "      <th>level0Name</th>\n",
       "      <th>level1Gid</th>\n",
       "      <th>level1Name</th>\n",
       "      <th>level2Gid</th>\n",
       "      <th>...</th>\n",
       "      <th>Substrate_text</th>\n",
       "      <th>Habitat_text</th>\n",
       "      <th>MetaSubstrate_text</th>\n",
       "      <th>kingdom_text</th>\n",
       "      <th>phylum_text</th>\n",
       "      <th>class_text</th>\n",
       "      <th>order_text</th>\n",
       "      <th>family_text</th>\n",
       "      <th>genus_text</th>\n",
       "      <th>species_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2238506390</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>DK</td>\n",
       "      <td>5179</td>\n",
       "      <td>9</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>28</td>\n",
       "      <td>Hovedstaden</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>soil</td>\n",
       "      <td>park/churchyard</td>\n",
       "      <td>jord</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>Basidiomycota</td>\n",
       "      <td>Agaricomycetes</td>\n",
       "      <td>Polyporales</td>\n",
       "      <td>Meruliaceae</td>\n",
       "      <td>Abortiporus</td>\n",
       "      <td>Abortiporus biennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2812984326</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>DK</td>\n",
       "      <td>6157</td>\n",
       "      <td>9</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>32</td>\n",
       "      <td>Syddanmark</td>\n",
       "      <td>146</td>\n",
       "      <td>...</td>\n",
       "      <td>dead wood (including bark)</td>\n",
       "      <td>lawn</td>\n",
       "      <td>wood</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>Basidiomycota</td>\n",
       "      <td>Agaricomycetes</td>\n",
       "      <td>Polyporales</td>\n",
       "      <td>Meruliaceae</td>\n",
       "      <td>Abortiporus</td>\n",
       "      <td>Abortiporus biennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2465026418</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>DK</td>\n",
       "      <td>6829</td>\n",
       "      <td>9</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>31</td>\n",
       "      <td>Sjælland</td>\n",
       "      <td>139</td>\n",
       "      <td>...</td>\n",
       "      <td>dead wood (including bark)</td>\n",
       "      <td>Unmanaged deciduous woodland</td>\n",
       "      <td>wood</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>Basidiomycota</td>\n",
       "      <td>Agaricomycetes</td>\n",
       "      <td>Polyporales</td>\n",
       "      <td>Meruliaceae</td>\n",
       "      <td>Abortiporus</td>\n",
       "      <td>Abortiporus biennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2812994333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>DK</td>\n",
       "      <td>6145</td>\n",
       "      <td>9</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>32</td>\n",
       "      <td>Syddanmark</td>\n",
       "      <td>146</td>\n",
       "      <td>...</td>\n",
       "      <td>dead wood (including bark)</td>\n",
       "      <td>lawn</td>\n",
       "      <td>wood</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>Basidiomycota</td>\n",
       "      <td>Agaricomycetes</td>\n",
       "      <td>Polyporales</td>\n",
       "      <td>Meruliaceae</td>\n",
       "      <td>Abortiporus</td>\n",
       "      <td>Abortiporus biennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2812983332</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>DK</td>\n",
       "      <td>6157</td>\n",
       "      <td>9</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>32</td>\n",
       "      <td>Syddanmark</td>\n",
       "      <td>146</td>\n",
       "      <td>...</td>\n",
       "      <td>dead wood (including bark)</td>\n",
       "      <td>lawn</td>\n",
       "      <td>wood</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>Basidiomycota</td>\n",
       "      <td>Agaricomycetes</td>\n",
       "      <td>Polyporales</td>\n",
       "      <td>Meruliaceae</td>\n",
       "      <td>Abortiporus</td>\n",
       "      <td>Abortiporus biennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356765</th>\n",
       "      <td>2238482337</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>DK</td>\n",
       "      <td>9621</td>\n",
       "      <td>9</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>29</td>\n",
       "      <td>Midtjylland</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "      <td>faeces</td>\n",
       "      <td>other habitat</td>\n",
       "      <td>animals</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>Basidiomycota</td>\n",
       "      <td>Agaricomycetes</td>\n",
       "      <td>Agaricales</td>\n",
       "      <td>Tricholomataceae</td>\n",
       "      <td>Clitocybe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356766</th>\n",
       "      <td>2237940567</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>DK</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>29</td>\n",
       "      <td>Midtjylland</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>faeces</td>\n",
       "      <td>salt meadow</td>\n",
       "      <td>animals</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>Basidiomycota</td>\n",
       "      <td>Agaricomycetes</td>\n",
       "      <td>Agaricales</td>\n",
       "      <td>Tricholomataceae</td>\n",
       "      <td>Clitocybe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356767</th>\n",
       "      <td>2238455560</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>DK</td>\n",
       "      <td>1662</td>\n",
       "      <td>9</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>29</td>\n",
       "      <td>Midtjylland</td>\n",
       "      <td>108</td>\n",
       "      <td>...</td>\n",
       "      <td>faeces</td>\n",
       "      <td>salt meadow</td>\n",
       "      <td>animals</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>Basidiomycota</td>\n",
       "      <td>Agaricomycetes</td>\n",
       "      <td>Agaricales</td>\n",
       "      <td>Tricholomataceae</td>\n",
       "      <td>Clitocybe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356768</th>\n",
       "      <td>2449442140</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>DK</td>\n",
       "      <td>6116</td>\n",
       "      <td>9</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>31</td>\n",
       "      <td>Sjælland</td>\n",
       "      <td>133</td>\n",
       "      <td>...</td>\n",
       "      <td>faeces</td>\n",
       "      <td>Forest bog</td>\n",
       "      <td>animals</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>Basidiomycota</td>\n",
       "      <td>Agaricomycetes</td>\n",
       "      <td>Agaricales</td>\n",
       "      <td>Tricholomataceae</td>\n",
       "      <td>Clitocybe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356769</th>\n",
       "      <td>2238127922</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>DK</td>\n",
       "      <td>9056</td>\n",
       "      <td>9</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>31</td>\n",
       "      <td>Sjælland</td>\n",
       "      <td>130</td>\n",
       "      <td>...</td>\n",
       "      <td>faeces</td>\n",
       "      <td>meadow</td>\n",
       "      <td>animals</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>Basidiomycota</td>\n",
       "      <td>Agaricomycetes</td>\n",
       "      <td>Agaricales</td>\n",
       "      <td>Tricholomataceae</td>\n",
       "      <td>Clitocybe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356770 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        observationID  month   day countryCode  locality  level0Gid  \\\n",
       "0          2238506390    9.0   1.0          DK      5179          9   \n",
       "1          2812984326    7.0   8.0          DK      6157          9   \n",
       "2          2465026418   11.0  19.0          DK      6829          9   \n",
       "3          2812994333    7.0   9.0          DK      6145          9   \n",
       "4          2812983332    8.0  20.0          DK      6157          9   \n",
       "...               ...    ...   ...         ...       ...        ...   \n",
       "356765     2238482337   11.0  23.0          DK      9621          9   \n",
       "356766     2237940567   10.0  26.0          DK      2012          9   \n",
       "356767     2238455560   11.0   7.0          DK      1662          9   \n",
       "356768     2449442140   11.0   3.0          DK      6116          9   \n",
       "356769     2238127922   11.0  14.0          DK      9056          9   \n",
       "\n",
       "       level0Name  level1Gid   level1Name  level2Gid  ...  \\\n",
       "0         Denmark         28  Hovedstaden         80  ...   \n",
       "1         Denmark         32   Syddanmark        146  ...   \n",
       "2         Denmark         31     Sjælland        139  ...   \n",
       "3         Denmark         32   Syddanmark        146  ...   \n",
       "4         Denmark         32   Syddanmark        146  ...   \n",
       "...           ...        ...          ...        ...  ...   \n",
       "356765    Denmark         29  Midtjylland         98  ...   \n",
       "356766    Denmark         29  Midtjylland        111  ...   \n",
       "356767    Denmark         29  Midtjylland        108  ...   \n",
       "356768    Denmark         31     Sjælland        133  ...   \n",
       "356769    Denmark         31     Sjælland        130  ...   \n",
       "\n",
       "                    Substrate_text                  Habitat_text  \\\n",
       "0                             soil               park/churchyard   \n",
       "1       dead wood (including bark)                          lawn   \n",
       "2       dead wood (including bark)  Unmanaged deciduous woodland   \n",
       "3       dead wood (including bark)                          lawn   \n",
       "4       dead wood (including bark)                          lawn   \n",
       "...                            ...                           ...   \n",
       "356765                      faeces                 other habitat   \n",
       "356766                      faeces                   salt meadow   \n",
       "356767                      faeces                   salt meadow   \n",
       "356768                      faeces                    Forest bog   \n",
       "356769                      faeces                        meadow   \n",
       "\n",
       "        MetaSubstrate_text  kingdom_text    phylum_text      class_text  \\\n",
       "0                     jord         Fungi  Basidiomycota  Agaricomycetes   \n",
       "1                     wood         Fungi  Basidiomycota  Agaricomycetes   \n",
       "2                     wood         Fungi  Basidiomycota  Agaricomycetes   \n",
       "3                     wood         Fungi  Basidiomycota  Agaricomycetes   \n",
       "4                     wood         Fungi  Basidiomycota  Agaricomycetes   \n",
       "...                    ...           ...            ...             ...   \n",
       "356765             animals         Fungi  Basidiomycota  Agaricomycetes   \n",
       "356766             animals         Fungi  Basidiomycota  Agaricomycetes   \n",
       "356767             animals         Fungi  Basidiomycota  Agaricomycetes   \n",
       "356768             animals         Fungi  Basidiomycota  Agaricomycetes   \n",
       "356769             animals         Fungi  Basidiomycota  Agaricomycetes   \n",
       "\n",
       "         order_text       family_text   genus_text         species_text  \n",
       "0       Polyporales       Meruliaceae  Abortiporus  Abortiporus biennis  \n",
       "1       Polyporales       Meruliaceae  Abortiporus  Abortiporus biennis  \n",
       "2       Polyporales       Meruliaceae  Abortiporus  Abortiporus biennis  \n",
       "3       Polyporales       Meruliaceae  Abortiporus  Abortiporus biennis  \n",
       "4       Polyporales       Meruliaceae  Abortiporus  Abortiporus biennis  \n",
       "...             ...               ...          ...                  ...  \n",
       "356765   Agaricales  Tricholomataceae    Clitocybe                  NaN  \n",
       "356766   Agaricales  Tricholomataceae    Clitocybe                  NaN  \n",
       "356767   Agaricales  Tricholomataceae    Clitocybe                  NaN  \n",
       "356768   Agaricales  Tricholomataceae    Clitocybe                  NaN  \n",
       "356769   Agaricales  Tricholomataceae    Clitocybe                  NaN  \n",
       "\n",
       "[356770 rows x 44 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pairing up with embeddings, we will use numerical data only so there is less data to load etc\n",
    "numerical_metadata_df = selected_metadata_df.drop([c + \"_text\" for c in CATEGORICAL_COLUMNS], axis=1)\n",
    "numerical_metadata_df = numerical_metadata_df.drop(['filename', 'scientificName', 'countryCode', 'level0Name', 'level1Name', 'level2Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_metadata_df['image_path'] = numerical_metadata_df.image_path.apply(lambda x: x.replace(\".JPG\", \".jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Exception in thread \"main\" java.nio.file.NoSuchFileException: /tmp/tmpbzot30rp/connection9366208419788263865.info\n",
      "\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n",
      "\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\n",
      "\tat java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219)\n",
      "\tat java.base/java.nio.file.Files.newByteChannel(Files.java:371)\n",
      "\tat java.base/java.nio.file.Files.createFile(Files.java:648)\n",
      "\tat java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:137)\n",
      "\tat java.base/java.nio.file.TempFileHelper.createTempFile(TempFileHelper.java:160)\n",
      "\tat java.base/java.nio.file.Files.createTempFile(Files.java:868)\n",
      "\tat org.apache.spark.api.python.PythonGatewayServer$.main(PythonGatewayServer.scala:54)\n",
      "\tat org.apache.spark.api.python.PythonGatewayServer.main(PythonGatewayServer.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "24/04/24 14:26:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/04/24 14:26:09 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "24/04/24 14:26:22 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-google-hadoop-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Match resnet embeddings\n",
    "resnet_pq = \"gs://dsgt-clef-fungiclef-2024/data/parquet/DF20_300px_and_DF21_300px_corrected_FULL_SET_embedding/resnet\"\n",
    "spark = get_spark(**{\n",
    "    \"spark.sql.parquet.enableVectorizedReader\": False, \n",
    "})\n",
    "\n",
    "resnet_df = spark.read.parquet(resnet_pq)\n",
    "resnet_embeddings = resnet_df.select(\"image_path\", \"embeddings\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match dataset by image_path\n",
    "resnet_embeddings_full_df = numerical_metadata_df.set_index('image_path').join(resnet_embeddings.set_index('image_path')).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"gs://dsgt-clef-fungiclef-2024/production/resnet/\"\n",
    "resnet_embeddings_full_df[resnet_embeddings_full_df.dataset==\"train\"].to_parquet(DATASET_PATH + \"DF_300_train.parquet\")\n",
    "resnet_embeddings_full_df[resnet_embeddings_full_df.dataset==\"valid\"].to_parquet(DATASET_PATH + \"DF_300_valid.parquet\")\n",
    "resnet_embeddings_full_df[resnet_embeddings_full_df.dataset==\"test\"].to_parquet(DATASET_PATH + \"DF_300_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the same for dino embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match resnet embeddings\n",
    "resnet_pq = \"gs://dsgt-clef-fungiclef-2024/data/parquet/DF20_300px_and_DF21_300px_corrected_FULL_SET_embedding/resnet\"\n",
    "spark = get_spark(**{\n",
    "    \"spark.sql.parquet.enableVectorizedReader\": False, \n",
    "})\n",
    "\n",
    "resnet_df = spark.read.parquet(resnet_pq)\n",
    "resnet_embeddings = resnet_df.select(\"image_path\", \"embeddings\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dino embeddings not working out yet because of the bug. TODO: Re-do dino embeddings\n",
    "dino_dct_pq = \"gs://dsgt-clef-fungiclef-2024/data/parquet/DF20_300px_and_DF21_300px_corrected_FULL_SET_embedding/dino/data/sample_id=0/part-00000-bfd7deaf-5486-4cf3-b861-f52758aae09c-c000.snappy.parquet\"\n",
    "# spark = get_spark(**{\n",
    "#     \"spark.sql.parquet.enableVectorizedReader\": False, \n",
    "# })\n",
    "\n",
    "dino_dct_df = pd.read_parquet(dino_dct_pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.6364484,  0.0064761,  1.9875255, ..., -2.3837745, -0.7289071,\n",
       "       -1.0162587], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino_dct_df.dino_embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_dct_df.dino_embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import dctn, dct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /home/chris/.cache/torch/hub/main.zip\n",
      "/home/chris/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/chris/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/chris/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth\" to /home/chris/.cache/torch/hub/checkpoints/dinov2_vitl14_pretrain.pth\n",
      "100%|██████████| 1.13G/1.13G [00:04<00:00, 252MB/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([197376])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_1d = LayerNorm(dino_dct_df.dino_embedding[0].shape)\n",
    "norm_1d.forward(torch.Tensor(dino_dct_df.dino_embedding[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DinoVisionTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mDinoVisionTransformer\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DinoVisionTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "DinoVisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = dinov2_vitl14.forward(torch.rand((1, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6972, -2.8514,  3.2909,  ..., -0.6694,  0.3950, -1.0779],\n",
       "        [ 0.3097, -1.9966, -0.1062,  ..., -1.3243, -0.5795, -0.4068],\n",
       "        [ 0.2852,  0.3234, -0.4515,  ..., -0.7450,  0.5257,  2.0890],\n",
       "        ...,\n",
       "        [ 3.5287, -2.0530, -1.8518,  ..., -1.3409, -1.0811,  0.1767],\n",
       "        [ 0.5803,  0.8584, -1.0418,  ..., -0.3129, -0.0696,  2.6281],\n",
       "        [ 3.2293, -2.8874, -0.4942,  ..., -1.9794, -1.2602, -0.4612]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DINO_SHAPE=(257, 768)\n",
    "    \n",
    "def dctn_filter(tile, k):\n",
    "    coeff = dctn(tile.reshape(DINO_SHAPE))\n",
    "    coeff_subset = coeff[:k, :k]\n",
    "    return coeff_subset.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Norm the inputs here\n",
    "\n",
    "def process_hidden_states(df):\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        hidden_state = row.dino_embedding.reshape(DINO_SHAPE)\n",
    "        cls_token = hidden_state[0]\n",
    "        dct_16_1d = dct(hidden_state[1:], axis=-1)[:, :16]\n",
    "        dct_64_2d = dctn(hidden_state[1:])[:64, :64]\n",
    "        rows.append(dict(cls_token=cls_token.tolist(), dct_16_1d=dct_16_1d.tolist(), dct_64_2d=dct_64_2d.tolist()))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def list_blobs_with_prefix(bucket_name, prefix, delimiter=None):\n",
    "    \"\"\"Lists all the blobs in the bucket that begin with the prefix.\n",
    "\n",
    "    This can be used to list all blobs in a \"folder\", e.g. \"public/\".\n",
    "\n",
    "    The delimiter argument can be used to restrict the results to only the\n",
    "    \"files\" in the given \"folder\". Without the delimiter, the entire tree under\n",
    "    the prefix is returned. For example, given these blobs:\n",
    "\n",
    "        a/1.txt\n",
    "        a/b/2.txt\n",
    "\n",
    "    If you specify prefix ='a/', without a delimiter, you'll get back:\n",
    "\n",
    "        a/1.txt\n",
    "        a/b/2.txt\n",
    "\n",
    "    However, if you specify prefix='a/' and delimiter='/', you'll get back\n",
    "    only the file directly under 'a/':\n",
    "\n",
    "        a/1.txt\n",
    "\n",
    "    As part of the response, you'll also get back a blobs.prefixes entity\n",
    "    that lists the \"subfolders\" under `a/`:\n",
    "\n",
    "        a/b/\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix, delimiter=delimiter)\n",
    "\n",
    "    # Note: The call returns a response only when the iterator is consumed.\n",
    "\n",
    "    file_list = []\n",
    "    print(\"Blobs:\")\n",
    "    for blob in blobs:\n",
    "        if \".parquet\" in blob.name: \n",
    "            file_list.append(\"gs://\"+bucket_name+\"/\"+blob.name)\n",
    "\n",
    "    return file_list\n",
    "\n",
    "    # if delimiter:\n",
    "    #     print(\"Prefixes:\")\n",
    "    #     for prefix in blobs.prefixes:\n",
    "    #         print(prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blobs:\n"
     ]
    }
   ],
   "source": [
    "dino_outputs = list_blobs_with_prefix(\"dsgt-clef-fungiclef-2024\", prefix=\"data/parquet/DF20_300px_and_DF21_300px_corrected_FULL_SET_embedding/dino/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:32,  2.96s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_file(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    return process_hidden_states(df)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    all_rows = list(tqdm(executor.map(process_file, dino_outputs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 13/1000 [00:24<31:27,  1.91s/it] \n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process_hidden_states(df)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m---> 10\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdino_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdino_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     all_rows \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]  \u001b[38;5;66;03m# Flatten list if necessary\u001b[39;00m\n",
      "File \u001b[0;32m~/fungiclef-2024/.venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/process.py:575\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    576\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    return process_hidden_states(df)\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(process_file, dino_outputs), total=len(dino_outputs)))\n",
    "    all_rows = [item for sublist in results for item in sublist]  # Flatten list if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[236], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m dino_outputs:\n\u001b[0;32m----> 2\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     rows \u001b[38;5;241m=\u001b[39m process_hidden_states(df)\n",
      "File \u001b[0;32m~/fungiclef-2024/.venv/lib/python3.10/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fungiclef-2024/.venv/lib/python3.10/site-packages/pandas/io/parquet.py:281\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 281\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mto_pandas_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    284\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_as_manager(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/fungiclef-2024/.venv/lib/python3.10/site-packages/pyarrow/array.pxi:883\u001b[0m, in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/fungiclef-2024/.venv/lib/python3.10/site-packages/pyarrow/table.pxi:4251\u001b[0m, in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/fungiclef-2024/.venv/lib/python3.10/site-packages/pyarrow/pandas_compat.py:777\u001b[0m, in \u001b[0;36mtable_to_dataframe\u001b[0;34m(options, table, categories, ignore_metadata, types_mapper)\u001b[0m\n\u001b[1;32m    775\u001b[0m _check_data_column_metadata_consistency(all_columns)\n\u001b[1;32m    776\u001b[0m columns \u001b[38;5;241m=\u001b[39m _deserialize_column_index(table, all_columns, column_indexes)\n\u001b[0;32m--> 777\u001b[0m blocks \u001b[38;5;241m=\u001b[39m \u001b[43m_table_to_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mext_columns_dtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[1;32m    780\u001b[0m mgr \u001b[38;5;241m=\u001b[39m BlockManager(blocks, axes)\n",
      "File \u001b[0;32m~/fungiclef-2024/.venv/lib/python3.10/site-packages/pyarrow/pandas_compat.py:1131\u001b[0m, in \u001b[0;36m_table_to_blocks\u001b[0;34m(options, block_table, categories, extension_columns)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_table_to_blocks\u001b[39m(options, block_table, categories, extension_columns):\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;66;03m# Part of table_to_blockmanager\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Convert an arrow table to Block from the internal pandas API\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     columns \u001b[38;5;241m=\u001b[39m block_table\u001b[38;5;241m.\u001b[39mcolumn_names\n\u001b[0;32m-> 1131\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable_to_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextension_columns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_reconstruct_block(item, columns, extension_columns)\n\u001b[1;32m   1134\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "all_rows = []\n",
    "for file_path in dino_outputs:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    rows = process_hidden_states(df)\n",
    "    all_rows += rowsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(dino_outputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = process_hidden_states(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cls_token</th>\n",
       "      <th>dct_16_1d</th>\n",
       "      <th>dct_64_2d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3.636448383331299, 0.006476100534200668, 1.98...</td>\n",
       "      <td>[[-90.1867446899414, 39.47825622558594, 41.866...</td>\n",
       "      <td>[[-17564.009765625, 8145.9775390625, 28178.802...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.7649978399276733, 0.4228263795375824, 0.184...</td>\n",
       "      <td>[[-77.36708068847656, 11.427928924560547, -1.5...</td>\n",
       "      <td>[[-14039.01953125, 15973.794921875, -20988.708...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.9485918879508972, 2.36519718170166, 2.81349...</td>\n",
       "      <td>[[-88.82673645019531, -1.0302658081054688, 34....</td>\n",
       "      <td>[[-32892.84375, -14167.4970703125, 14405.75292...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-4.624961853027344, -0.2530408501625061, 1.81...</td>\n",
       "      <td>[[-27.530357360839844, -5.908840179443359, 44....</td>\n",
       "      <td>[[-21471.888671875, 17808.14453125, 11436.0507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2.610158920288086, -0.07085422426462173, -3.0...</td>\n",
       "      <td>[[-59.53972625732422, 52.84880447387695, 35.24...</td>\n",
       "      <td>[[-21902.71875, 9298.951171875, 16262.63085937...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>[0.9336028695106506, 0.5011065602302551, 0.471...</td>\n",
       "      <td>[[-86.13015747070312, 47.450103759765625, 37.5...</td>\n",
       "      <td>[[-18539.68359375, 30315.546875, 26999.9804687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>[2.7459166049957275, 1.9657036066055298, -1.76...</td>\n",
       "      <td>[[-89.84234619140625, 75.6082534790039, 17.731...</td>\n",
       "      <td>[[-20514.4453125, 16542.859375, 6334.349609375...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>[0.5621036887168884, -1.015462040901184, -1.71...</td>\n",
       "      <td>[[-87.16651916503906, 65.7000961303711, 37.032...</td>\n",
       "      <td>[[-26762.4609375, 15582.8291015625, 16527.1660...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>[1.960525631904602, -1.2922641038894653, -1.99...</td>\n",
       "      <td>[[5.040805816650391, 1.289764404296875, 29.879...</td>\n",
       "      <td>[[-22208.4921875, 9247.31640625, 18881.9042968...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>[-1.7993268966674805, 0.5162307024002075, -0.4...</td>\n",
       "      <td>[[-96.54596710205078, 38.60463333129883, 27.74...</td>\n",
       "      <td>[[-20622.896484375, 27090.58984375, 8670.19628...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>343 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             cls_token  \\\n",
       "0    [3.636448383331299, 0.006476100534200668, 1.98...   \n",
       "1    [0.7649978399276733, 0.4228263795375824, 0.184...   \n",
       "2    [0.9485918879508972, 2.36519718170166, 2.81349...   \n",
       "3    [-4.624961853027344, -0.2530408501625061, 1.81...   \n",
       "4    [2.610158920288086, -0.07085422426462173, -3.0...   \n",
       "..                                                 ...   \n",
       "338  [0.9336028695106506, 0.5011065602302551, 0.471...   \n",
       "339  [2.7459166049957275, 1.9657036066055298, -1.76...   \n",
       "340  [0.5621036887168884, -1.015462040901184, -1.71...   \n",
       "341  [1.960525631904602, -1.2922641038894653, -1.99...   \n",
       "342  [-1.7993268966674805, 0.5162307024002075, -0.4...   \n",
       "\n",
       "                                             dct_16_1d  \\\n",
       "0    [[-90.1867446899414, 39.47825622558594, 41.866...   \n",
       "1    [[-77.36708068847656, 11.427928924560547, -1.5...   \n",
       "2    [[-88.82673645019531, -1.0302658081054688, 34....   \n",
       "3    [[-27.530357360839844, -5.908840179443359, 44....   \n",
       "4    [[-59.53972625732422, 52.84880447387695, 35.24...   \n",
       "..                                                 ...   \n",
       "338  [[-86.13015747070312, 47.450103759765625, 37.5...   \n",
       "339  [[-89.84234619140625, 75.6082534790039, 17.731...   \n",
       "340  [[-87.16651916503906, 65.7000961303711, 37.032...   \n",
       "341  [[5.040805816650391, 1.289764404296875, 29.879...   \n",
       "342  [[-96.54596710205078, 38.60463333129883, 27.74...   \n",
       "\n",
       "                                             dct_64_2d  \n",
       "0    [[-17564.009765625, 8145.9775390625, 28178.802...  \n",
       "1    [[-14039.01953125, 15973.794921875, -20988.708...  \n",
       "2    [[-32892.84375, -14167.4970703125, 14405.75292...  \n",
       "3    [[-21471.888671875, 17808.14453125, 11436.0507...  \n",
       "4    [[-21902.71875, 9298.951171875, 16262.63085937...  \n",
       "..                                                 ...  \n",
       "338  [[-18539.68359375, 30315.546875, 26999.9804687...  \n",
       "339  [[-20514.4453125, 16542.859375, 6334.349609375...  \n",
       "340  [[-26762.4609375, 15582.8291015625, 16527.1660...  \n",
       "341  [[-22208.4921875, 9247.31640625, 18881.9042968...  \n",
       "342  [[-20622.896484375, 27090.58984375, 8670.19628...  \n",
       "\n",
       "[343 rows x 3 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>species</th>\n",
       "      <th>dino_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2237912865-151073.jpg</td>\n",
       "      <td>Hortiboletus engelii</td>\n",
       "      <td>[3.6364484, 0.0064761005, 1.9875255, 2.2292707...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2238406072-312936.jpg</td>\n",
       "      <td>Peniophora rufomarginata</td>\n",
       "      <td>[0.76499784, 0.42282638, 0.18455583, -0.292270...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2238556407-330535.jpg</td>\n",
       "      <td>Hortiboletus engelii</td>\n",
       "      <td>[0.9485919, 2.3651972, 2.8134954, -0.26696065,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2238566015-183150.jpg</td>\n",
       "      <td>Calvatia gigantea</td>\n",
       "      <td>[-4.624962, -0.25304085, 1.8104588, -1.1925237...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2238579738-37528.jpg</td>\n",
       "      <td>Panaeolus subfirmus</td>\n",
       "      <td>[2.610159, -0.070854224, -3.038675, 0.9941942,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0-3355971447.jpg</td>\n",
       "      <td>Hygrophoropsis aurantiaca</td>\n",
       "      <td>[0.93360287, 0.50110656, 0.47133487, -0.294665...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0-3126935330.jpg</td>\n",
       "      <td>Cladonia crispata</td>\n",
       "      <td>[2.7459166, 1.9657036, -1.762023, 0.004444801,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>3-3395916415.jpg</td>\n",
       "      <td>Cuphophyllus colemannianus</td>\n",
       "      <td>[0.5621037, -1.015462, -1.7117326, 2.0356944, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0-3386488332.jpg</td>\n",
       "      <td>Lacrymaria lacrymabunda</td>\n",
       "      <td>[1.9605256, -1.2922641, -1.9904997, 1.3893987,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>2-3419929368.jpg</td>\n",
       "      <td>Hemitrichia clavata</td>\n",
       "      <td>[-1.7993269, 0.5162307, -0.46492264, -2.422506...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>343 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                image_path                     species  \\\n",
       "0    2237912865-151073.jpg        Hortiboletus engelii   \n",
       "1    2238406072-312936.jpg    Peniophora rufomarginata   \n",
       "2    2238556407-330535.jpg        Hortiboletus engelii   \n",
       "3    2238566015-183150.jpg           Calvatia gigantea   \n",
       "4     2238579738-37528.jpg         Panaeolus subfirmus   \n",
       "..                     ...                         ...   \n",
       "338       0-3355971447.jpg   Hygrophoropsis aurantiaca   \n",
       "339       0-3126935330.jpg           Cladonia crispata   \n",
       "340       3-3395916415.jpg  Cuphophyllus colemannianus   \n",
       "341       0-3386488332.jpg     Lacrymaria lacrymabunda   \n",
       "342       2-3419929368.jpg         Hemitrichia clavata   \n",
       "\n",
       "                                        dino_embedding  \n",
       "0    [3.6364484, 0.0064761005, 1.9875255, 2.2292707...  \n",
       "1    [0.76499784, 0.42282638, 0.18455583, -0.292270...  \n",
       "2    [0.9485919, 2.3651972, 2.8134954, -0.26696065,...  \n",
       "3    [-4.624962, -0.25304085, 1.8104588, -1.1925237...  \n",
       "4    [2.610159, -0.070854224, -3.038675, 0.9941942,...  \n",
       "..                                                 ...  \n",
       "338  [0.93360287, 0.50110656, 0.47133487, -0.294665...  \n",
       "339  [2.7459166, 1.9657036, -1.762023, 0.004444801,...  \n",
       "340  [0.5621037, -1.015462, -1.7117326, 2.0356944, ...  \n",
       "341  [1.9605256, -1.2922641, -1.9904997, 1.3893987,...  \n",
       "342  [-1.7993269, 0.5162307, -0.46492264, -2.422506...  \n",
       "\n",
       "[343 rows x 3 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356770"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_full_img_paths.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
