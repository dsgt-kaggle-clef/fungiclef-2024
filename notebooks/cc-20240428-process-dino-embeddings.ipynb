{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.fftpack import dctn, dct\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def list_blobs_with_prefix(bucket_name, prefix, delimiter=None):\n",
    "    \"\"\"Lists all the blobs in the bucket that begin with the prefix.\n",
    "\n",
    "    This can be used to list all blobs in a \"folder\", e.g. \"public/\".\n",
    "\n",
    "    The delimiter argument can be used to restrict the results to only the\n",
    "    \"files\" in the given \"folder\". Without the delimiter, the entire tree under\n",
    "    the prefix is returned. For example, given these blobs:\n",
    "\n",
    "        a/1.txt\n",
    "        a/b/2.txt\n",
    "\n",
    "    If you specify prefix ='a/', without a delimiter, you'll get back:\n",
    "\n",
    "        a/1.txt\n",
    "        a/b/2.txt\n",
    "\n",
    "    However, if you specify prefix='a/' and delimiter='/', you'll get back\n",
    "    only the file directly under 'a/':\n",
    "\n",
    "        a/1.txt\n",
    "\n",
    "    As part of the response, you'll also get back a blobs.prefixes entity\n",
    "    that lists the \"subfolders\" under `a/`:\n",
    "\n",
    "        a/b/\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix, delimiter=delimiter)\n",
    "\n",
    "    # Note: The call returns a response only when the iterator is consumed.\n",
    "\n",
    "    file_list = []\n",
    "    for blob in blobs:\n",
    "        if \".parquet\" in blob.name: \n",
    "            file_list.append(\"gs://\"+bucket_name+\"/\"+blob.name)\n",
    "\n",
    "    return file_list\n",
    "\n",
    "dino_outputs = list_blobs_with_prefix(\"dsgt-clef-fungiclef-2024\", prefix=\"data/parquet/DF20_300px_and_DF21_300px_corrected_FULL_SET_embedding/dino/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(dino_outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DINO_SHAPE=(257, 768)\n",
    "\n",
    "def process_hidden_states(df):\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        hidden_state = row.dino_embedding.reshape(DINO_SHAPE)\n",
    "        cls_token = hidden_state[0]\n",
    "        # dct_16_1d = dct(hidden_state[1:], axis=-1)[:, :16]\n",
    "        # dct_64_2d = dctn(hidden_state[1:])[:64, :64]\n",
    "        # rows.append(dict(cls_token=cls_token.tolist(), dct_16_1d=dct_16_1d.tolist(), dct_64_2d=dct_64_2d.tolist()))\n",
    "        rows.append({'image_path': row.image_path, 'dino_output': cls_token.tolist()})\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 48/1000 [01:02<22:39,  1.43s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def process_file(file_path):\n",
    "#     df = pd.read_parquet(file_path)\n",
    "#     _df = pd.DataFrame([df.image_path, df.dino_embedding.apply(lambda x: x[:768])]).T\n",
    "#     return _df\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "#     all_rows = list(tqdm(executor.map(process_file, dino_outputs), total=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [51:04<00:00,  3.06s/it] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_df = []\n",
    "for ix, file_path in enumerate(tqdm(dino_outputs)):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    _df = pd.DataFrame([df.image_path, df.dino_embedding.apply(lambda x: x[:768].tolist())]).T\n",
    "    _df.to_csv(f\"./tmp/dino_cls_{ix}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:54<00:00, 18.50it/s]\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "for i in tqdm(range(1000)):\n",
    "    df = pd.read_csv(f'./tmp/dino_cls_{i}.csv')\n",
    "    all_dfs.append(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"dino_embeddings_all.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/fungiclef-2024/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fungiclef.utils import get_spark, spark_resource, read_config\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config(path='../fungiclef/config.json')\n",
    "\n",
    "# First, we read the metadata for the dataset and make a proper new one. This will be the single source of truth we use to build the rest of our stuff on\n",
    "# This corresponds to the DF20 dataset\n",
    "TRAIN_METADATA = config[\"gs_paths\"][\"train\"][\"metadata\"]\n",
    "\n",
    "# These two correspond to the DF21 dataset\n",
    "VALID_METADATA = config[\"gs_paths\"][\"val\"][\"metadata\"]\n",
    "TEST_METADATA = config[\"gs_paths\"][\"test\"][\"metadata\"]\n",
    "\n",
    "PRODUCTION_BUCKET = 'gs://dsgt-clef-fungiclef-2024/production/'\n",
    "\n",
    "# Here, we are only keeping columns that are relevant either for training or inference. \n",
    "# This includes all the columns that were present in the public test metadata dataset\n",
    "TEST_DF_COLUMNS = ['observationID', 'month', 'day', 'countryCode', 'locality', 'level0Gid',\n",
    "       'level0Name', 'level1Gid', 'level1Name', 'level2Gid', 'level2Name',\n",
    "       'Substrate', 'Latitude', 'Longitude', 'CoorUncert', 'Habitat',\n",
    "       'image_path', 'filename', 'MetaSubstrate']\n",
    "\n",
    "# As well as the overall classification of the fungi (this could potentially be useful as additional training targets)\n",
    "COLUMNS_TO_KEEP = TEST_DF_COLUMNS + ['scientificName', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 'poisonous', 'class_id', 'dataset']\n",
    "\n",
    "# These are the categorical columns we will need to factorize and generate labels for\n",
    "CATEGORICAL_COLUMNS = ['locality', 'level0Gid', 'level1Gid', 'level2Gid', 'Substrate', 'Habitat', 'MetaSubstrate', 'kingdom', 'phylum', 'class',\n",
    "       'order', 'family', 'genus', 'species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metadata_df = pd.read_csv(PRODUCTION_BUCKET + \"metadata/DF_combined_metadata_mapped_columns.csv\")\n",
    "# For pairing up with embeddings, we will use numerical data only so there is less data to load etc\n",
    "numerical_metadata_df = selected_metadata_df.drop([c + \"_text\" for c in CATEGORICAL_COLUMNS], axis=1)\n",
    "numerical_metadata_df = numerical_metadata_df.drop(['filename', 'scientificName', 'countryCode', 'level0Name', 'level1Name', 'level2Name'], axis=1)\n",
    "\n",
    "numerical_metadata_df['image_path'] = numerical_metadata_df.image_path.apply(lambda x: x.replace(\".JPG\", \".jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match dataset by image_path\n",
    "dino_full_df = numerical_metadata_df.set_index('image_path').join(df.set_index('image_path')).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"gs://dsgt-clef-fungiclef-2024/production/dino_cls/\"\n",
    "dino_full_df[dino_full_df.dataset==\"train\"].to_parquet(DATASET_PATH + \"DF_300_train.parquet\")\n",
    "dino_full_df[dino_full_df.dataset==\"valid\"].to_parquet(DATASET_PATH + \"DF_300_valid.parquet\")\n",
    "dino_full_df[dino_full_df.dataset==\"test\"].to_parquet(DATASET_PATH + \"DF_300_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
